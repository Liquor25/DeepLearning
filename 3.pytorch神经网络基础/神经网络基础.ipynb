{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 层和块\n",
    "#### 首先，回顾一下多层感知机"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6982, 0.2854, 0.0290, 0.7186, 0.0909, 0.6777, 0.8265, 0.5853, 0.7808,\n",
      "         0.5695, 0.8137, 0.1170, 0.5396, 0.8231, 0.2252, 0.2300, 0.7124, 0.4999,\n",
      "         0.1046, 0.4006],\n",
      "        [0.3517, 0.9393, 0.3400, 0.3913, 0.5782, 0.0842, 0.9048, 0.2108, 0.1988,\n",
      "         0.1807, 0.5425, 0.7421, 0.3162, 0.5529, 0.9269, 0.2348, 0.2071, 0.1541,\n",
      "         0.9291, 0.3744]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2138, -0.2288, -0.0046, -0.0808,  0.3115, -0.2133, -0.0592,  0.0755,\n",
       "          0.2101, -0.0441],\n",
       "        [ 0.3398, -0.2338, -0.0798, -0.1090,  0.4404,  0.0276, -0.0327,  0.0859,\n",
       "          0.2104,  0.0121]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# nn.Sequential 定义了一种特殊的Module\n",
    "net = nn.Sequential(nn.Linear(20,256),nn.ReLU(),nn.Linear(256,10)) \n",
    "\n",
    "x = torch.rand(2,20)\n",
    "print(x)\n",
    "net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 自定义块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0729, -0.0254,  0.0320,  0.2166, -0.1733,  0.0527, -0.2350, -0.1192,\n",
       "         -0.0709, -0.2738],\n",
       "        [-0.1414, -0.0980,  0.1425,  0.1301, -0.1586, -0.0246, -0.4771, -0.0048,\n",
       "         -0.1115, -0.3160]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 自定义一个MLP\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(20,256) # 隐藏层\n",
    "        self.out = nn.Linear(256,10)    # 输出层\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return self.out(F.relu(self.hidden(x)))\n",
    "    \n",
    "net = MLP()\n",
    "net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 顺序块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0018,  0.1212,  0.3529, -0.1704,  0.0961,  0.2580,  0.1898, -0.0875,\n",
       "          0.1653, -0.2605],\n",
       "        [ 0.0881,  0.0433,  0.2426, -0.2055,  0.0274,  0.1916,  0.0390,  0.1200,\n",
       "          0.0262, -0.2085]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 自定义Sequential\n",
    "\"\"\"好处：可以在init和forward中做大量的自定义计算\"\"\"\n",
    "class MySequential(nn.Module):\n",
    "    def __init__(self,*args):\n",
    "        super().__init__()\n",
    "        for block in args:\n",
    "            self._modules[block] = block\n",
    "    \n",
    "    def forward(self,x):\n",
    "        for block in self._modules.values():\n",
    "            x = block(x)\n",
    "        return x\n",
    "net = MySequential(nn.Linear(20,256),nn.ReLU(),nn.Linear(256,10)) \n",
    "net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参数管理\n",
    "#### 首先关注具有单隐藏层的MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5272, 0.1047, 0.4811, 0.8123],\n",
      "        [0.7596, 0.0793, 0.8465, 0.8960]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.2356],\n",
       "        [0.2899]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "net = nn.Sequential(nn.Linear(4,8),nn.ReLU(),nn.Linear(8,1)) \n",
    "\n",
    "x = torch.rand(size=(2,4)) \n",
    "print(x)\n",
    "net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参数访问"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('weight', tensor([[ 0.3103, -0.2626, -0.0823,  0.0540, -0.1214, -0.2585,  0.1806,  0.2616]])), ('bias', tensor([0.1019]))])\n",
      "OrderedDict()\n",
      "OrderedDict([('weight', tensor([[ 0.3585, -0.4571,  0.1050,  0.4415],\n",
      "        [-0.0475, -0.1293, -0.2974, -0.1900],\n",
      "        [-0.2959, -0.4203, -0.2763,  0.4588],\n",
      "        [ 0.3156, -0.4127, -0.3548, -0.3061],\n",
      "        [ 0.4890, -0.3549, -0.1819, -0.4735],\n",
      "        [ 0.2545,  0.3148, -0.1795,  0.1740],\n",
      "        [-0.0910, -0.0700,  0.1461,  0.2510],\n",
      "        [ 0.1270, -0.1825, -0.3972,  0.4853]])), ('bias', tensor([-0.1729, -0.4285,  0.3623, -0.4897, -0.3994,  0.3708,  0.4954,  0.0322]))])\n"
     ]
    }
   ],
   "source": [
    "print(net[2].state_dict())  # 对应Sequential 中的nn.Linear(8,1)的权重\n",
    "print(net[1].state_dict())  # nn.ReLU()\n",
    "print(net[0].state_dict())  #对应Sequential 中的nn.Linear(4,8)的权重"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 目标参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.nn.parameter.Parameter'>\n",
      "Parameter containing:\n",
      "tensor([0.1019], requires_grad=True)\n",
      "tensor([0.1019])\n"
     ]
    }
   ],
   "source": [
    "print(type(net[2].bias))\n",
    "print(net[2].bias)\n",
    "print(net[2].bias.data) # 通过 .data 来访问这个点对应的值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[2].weight.grad == None # .grad 是获取梯度的，这里因为还没有进行反向计算，所以grad=None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 一次性访问所有参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('weight', torch.Size([8, 4])) ('bias', torch.Size([8]))\n",
      "('0.weight', torch.Size([8, 4])) ('0.bias', torch.Size([8])) ('2.weight', torch.Size([1, 8])) ('2.bias', torch.Size([1]))\n"
     ]
    }
   ],
   "source": [
    "print(*[(name,param.shape) for name,param in net[0].named_parameters()])\n",
    "print(*[(name,param.shape) for name,param in net.named_parameters()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 从嵌套块收集参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3437],\n",
       "        [0.3437]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def block1():\n",
    "    return nn.Sequential(nn.Linear(4,8),nn.ReLU(),nn.Linear(8,4),nn.ReLU())\n",
    "\n",
    "def block2():\n",
    "    net = nn.Sequential()\n",
    "    for i in range(4):\n",
    "        net.add_module(f'block{i}',block1())\n",
    "    return net\n",
    "rgnet = nn.Sequential(block2(),nn.Linear(4,1))\n",
    "rgnet(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Sequential(\n",
      "    (block0): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (block1): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (block2): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (block3): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (1): Linear(in_features=4, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(rgnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 内置初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.0103, 0.0016, 0.0021, 0.0064]), tensor(0.))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将参数修改成一个正态分布的数据\n",
    "def init_normal(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.normal_(m.weight,mean=0,std=0.01)\n",
    "        nn.init.zeros_(m.bias)\n",
    "        \n",
    "net.apply(init_normal)\n",
    "net[0].weight.data[0],net[0].bias.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1., 1., 1., 1.]), tensor(0.))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_constant(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.constant_(m.weight,1)\n",
    "        nn.init.zeros_(m.bias)\n",
    "        \n",
    "net.apply(init_constant)\n",
    "net[0].weight.data[0],net[0].bias.data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 对某些块采用不同的初始化方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.3874,  0.0306, -0.3589, -0.3128],\n",
      "        [-0.4779,  0.1630, -0.7012,  0.2944],\n",
      "        [ 0.5970,  0.1403, -0.3848, -0.4583],\n",
      "        [-0.2186,  0.5004, -0.7043, -0.3721],\n",
      "        [-0.6522,  0.5792, -0.6901, -0.2923],\n",
      "        [-0.4088,  0.4512, -0.4216, -0.1689],\n",
      "        [-0.0648,  0.2056, -0.6500, -0.4567],\n",
      "        [ 0.2559, -0.0684, -0.3333,  0.3479]])\n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "def xavier(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "        \n",
    "def init_constant(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.constant_(m.weight,1)\n",
    "        nn.init.zeros_(m.bias)\n",
    "        \n",
    "net[0].apply(xavier) \n",
    "net[2].apply(init_constant)\n",
    "print(net[0].weight.data)\n",
    "print(net[2].weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.6126, 3.0306, 2.6411, 2.6872])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 可以直接把值拿出来进行修改\n",
    "net[0].weight.data[:]+=1\n",
    "net[0].weight.data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参数绑定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([True, True, True, True, True, True, True, True])\n",
      "tensor([True, True, True, True, True, True, True, True])\n"
     ]
    }
   ],
   "source": [
    "# 两个数据流共享参数\n",
    "shared = nn.Linear(8,8)\n",
    "net = nn.Sequential(nn.Linear(4,8),nn.ReLU(),shared,nn.ReLU(),shared,nn.ReLU(),nn.Linear(8,1))\n",
    "\n",
    "net(x)\n",
    "print(net[2].weight.data[0]==net[4].weight.data[0])\n",
    "net[2].weight.data[0,0] = 100\n",
    "print(net[2].weight.data[0]==net[4].weight.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 自定义层\n",
    "#### 构造一个没有任何参数的自定义层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2., -1.,  0.,  1.,  2.])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class CenteredLayer(nn.Module):\n",
    "    def __init__(self,*args):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return x-x.mean()\n",
    "    \n",
    "layer = CenteredLayer()\n",
    "layer(torch.FloatTensor([1,2,3,4,5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 带参数的图层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.6420,  0.3398,  0.7631],\n",
       "        [ 0.1726, -1.3645,  0.4012],\n",
       "        [-2.1676, -1.2356, -0.1038],\n",
       "        [-0.5522,  0.2644, -1.2740],\n",
       "        [-1.7354,  0.8075,  1.0103]], requires_grad=True)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MyLinear(nn.Module):\n",
    "    def __init__(self,in_units,units):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.randn(in_units,units))\n",
    "        self.bias = nn.Parameter(torch.randn(units,))\n",
    "        \n",
    "    def forward(self,x):\n",
    "        linear = torch.matmul(x,self.weight.data)+self.bias.data\n",
    "        return F.relu(linear)\n",
    "    \n",
    "dense = MyLinear(5,3)\n",
    "dense.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 读写文件\n",
    "#### 加载和保存张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "x = torch.arange(4)\n",
    "torch.save(x,'x-file')\n",
    "\n",
    "x2 = torch.load(\"x-file\")\n",
    "x2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 加载和保存模型参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 自定义一个MLP\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(20,256) # 隐藏层\n",
    "        self.out = nn.Linear(256,10)    # 输出层\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return self.out(F.relu(self.hidden(x)))\n",
    "    \n",
    "net = MLP()\n",
    "x = torch.randn(size=(2,20))\n",
    "y = net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 将模型的参数存储为一个叫做“mlp.params”的文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 将模型的参数保存到字典里\n",
    "torch.save(net.state_dict(),'mlp.params')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (hidden): Linear(in_features=20, out_features=256, bias=True)\n",
       "  (out): Linear(in_features=256, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 实例化了原始多层感知机模型的一个备份，直接读取文件中存储的参数\n",
    "clone = MLP()\n",
    "clone.load_state_dict(torch.load(\"mlp.params\"))\n",
    "clone.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
